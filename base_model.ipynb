{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### CADQuery Code Generation - Notebook Summary\n",
    "\n",
    "#### 1. Objective\n",
    "\n",
    "Generate CadQuery code from rendered 2D images using a vision-to-language model.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Dataset\n",
    "\n",
    "* `CADCODER/GenCAD-Code`\n",
    "* 147K pairs of rendered image and corresponding CadQuery script.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Baseline Model\n",
    "\n",
    "* **CLIP (ViT-B/16)** as image encoder (frozen)\n",
    "* **GPT2** as language decoder (frozen)\n",
    "* **Projection Head**: MLP to map CLIP features to GPT2 token embeddings\n",
    "* **Training**: CrossEntropy loss on code tokens\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Enhanced Model\n",
    "\n",
    "* **Beam search decoding**: `num_beams=5`, `temperature=0.7`, `top_p=0.95`\n",
    "* **Feedback signal**: Valid syntax, executability, and geometry IOU\n",
    "* **PPO Reinforcement**: Reward-weighted training using PPO\n",
    "* **Optional geometry loss**: SDF/mesh loss placeholder\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Evaluation Metrics\n",
    "\n",
    "* **Valid Syntax Rate (VSR)**\n",
    "* **IOU\\_best**: 3D shape overlap via voxelization\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. Outcome\n",
    "\n",
    "* Both baseline and enhanced models implemented\n",
    "* Reinforcement and feedback-ready pipeline\n",
    "* Evaluation setup for fair comparison\n",
    "\n",
    "---\n",
    "\n",
    "> Full implementation details, design choices, and results are documented in the [README](./README.md).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses/mesh_loss.py\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from cadquery import exporters\n",
    "from cadquery import cq\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "def dummy_sdf_generator(code):\n",
    "    # Placeholder: create a fake voxel grid for now\n",
    "    # In practice, this should voxelize the actual CAD geometry\n",
    "    return torch.rand((32, 32, 32))\n",
    "\n",
    "def mesh_loss(pred_code: str, gt_sdf: torch.Tensor) -> torch.Tensor:\n",
    "    try:\n",
    "        sdf_pred = dummy_sdf_generator(pred_code)\n",
    "        sdf_pred = sdf_pred.to(gt_sdf.device)\n",
    "        return F.mse_loss(sdf_pred, gt_sdf)\n",
    "    except Exception as e:\n",
    "        print(\"Mesh loss generation failed:\", e)\n",
    "        return torch.tensor(0.0, requires_grad=True, device=gt_sdf.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rl/ppo.py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# You may need to import clip_model and clip_processor externally or pass as argument\n",
    "\n",
    "class PPOTrainer:\n",
    "    def __init__(self, model, proj, tokenizer, reward_fn, clip_model, clip_processor, device):\n",
    "        self.model = model\n",
    "        self.proj = proj\n",
    "        self.tokenizer = tokenizer\n",
    "        self.reward_fn = reward_fn\n",
    "        self.clip_model = clip_model\n",
    "        self.clip_processor = clip_processor\n",
    "        self.device = device\n",
    "\n",
    "        self.optimizer = AdamW(list(model.parameters()) + list(proj.parameters()), lr=1e-5)\n",
    "\n",
    "    def step(self, images, reward):\n",
    "        self.model.eval()\n",
    "        self.proj.eval()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            processed = self.clip_processor(images=images, return_tensors=\"pt\", padding=True).to(self.device)\n",
    "            clip_out = self.clip_model.get_image_features(**processed)\n",
    "            img_emb = self.proj(clip_out)\n",
    "            prefix_emb = img_emb.unsqueeze(1)\n",
    "\n",
    "        input_embeds = prefix_emb\n",
    "        outputs = self.model(inputs_embeds=input_embeds, labels=None)\n",
    "        logits = outputs.logits\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        # Simple scalar reward signal applied to log-probabilities\n",
    "        loss = -reward * log_probs.mean()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (\n",
    "    CLIPProcessor, CLIPModel,\n",
    "    GPT2Tokenizer, GPT2LMHeadModel,\n",
    "    AdamW, get_linear_schedule_with_warmup\n",
    ")\n",
    "from cadquery import exporters\n",
    "from metrics.best_iou import get_iou_best\n",
    "from metrics.valid_syntax_rate import evaluate_syntax_rate\n",
    "from datasets import load_dataset\n",
    "import ast\n",
    "import wandb\n",
    "from PIL import Image\n",
    "import io\n",
    "from torchvision import transforms\n",
    "\n",
    "# Optional: For SDF/mesh-based loss and PPO\n",
    "# from losses.mesh_loss import mesh_loss\n",
    "# from rl.ppo import PPOTrainer\n",
    "\n",
    "# ------------------- WandB -------------------\n",
    "# wandb.init(project=\"cadquery-codegen\", name=\"clip-gpt2-enhanced\")\n",
    "\n",
    "# ------------------- Models -------------------\n",
    "class ClipToGPT2Improved(nn.Module):\n",
    "    def __init__(self, clip_dim, gpt2_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(clip_dim, 4 * gpt2_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(4 * gpt2_dim, gpt2_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.proj(x)\n",
    "\n",
    "# ------------------- Utilities -------------------\n",
    "def is_valid_python(code):\n",
    "    try:\n",
    "        ast.parse(code)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def is_executable(code):\n",
    "    try:\n",
    "        exec_globals = {}\n",
    "        exec(code, exec_globals)\n",
    "        return exec_globals.get(\"result\", None) is not None\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# ------------------- Dataset -------------------\n",
    "class ImageCodeDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=256):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        img_data = item[\"image\"]\n",
    "\n",
    "        if isinstance(img_data, dict) and \"bytes\" in img_data:\n",
    "            image = Image.open(io.BytesIO(img_data[\"bytes\"])).convert(\"RGB\")\n",
    "        elif isinstance(img_data, str):\n",
    "            image = Image.open(img_data).convert(\"RGB\")\n",
    "        elif isinstance(img_data, Image.Image):\n",
    "            image = img_data.convert(\"RGB\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown image format: {type(img_data)}\")\n",
    "\n",
    "        image_tensor = self.transform(image)\n",
    "\n",
    "        code = item[\"cadquery\"]\n",
    "        code_ids = self.tokenizer(code, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            \"image\": image_tensor,\n",
    "            \"input_ids\": code_ids[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": code_ids[\"attention_mask\"].squeeze(0),\n",
    "            \"code\": code\n",
    "        }\n",
    "\n",
    "# ------------------- Load Models -------------------\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "gpt2.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "proj = ClipToGPT2Improved(clip_model.config.projection_dim, gpt2.config.n_embd)\n",
    "\n",
    "# ------------------- Device Setup -------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpt2.to(device)\n",
    "clip_model.to(device)\n",
    "proj.to(device)\n",
    "\n",
    "# ------------------- Optimizer -------------------\n",
    "optimizer = AdamW(list(gpt2.parameters()) + list(proj.parameters()), lr=5e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=1000)\n",
    "\n",
    "# ------------------- Flags -------------------\n",
    "use_feedback = True\n",
    "use_sdf_loss = True\n",
    "use_ppo = True\n",
    "ppo_trainer = PPOTrainer(gpt2, proj, tokenizer, reward_fn=get_iou_best)\n",
    "\n",
    "# ------------------- Load Dataset -------------------\n",
    "ds = load_dataset(\"CADCODER/GenCAD-Code\", cache_dir=\"/tmp/hf_cache\")\n",
    "train_dataset = ImageCodeDataset(ds[\"train\"], tokenizer)\n",
    "test_dataset = ImageCodeDataset(ds[\"test\"], tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "# ------------------- Training -------------------\n",
    "def train(train_loader):\n",
    "    gpt2.train()\n",
    "    proj.train()\n",
    "\n",
    "    for epoch in range(5):\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            images = batch[\"image\"].to(device)\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                processed = clip_processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n",
    "                clip_out = clip_model.get_image_features(**processed)\n",
    "\n",
    "            img_emb = proj(clip_out)\n",
    "            prefix_emb = img_emb.unsqueeze(1)\n",
    "\n",
    "            code_emb = gpt2.transformer.wte(input_ids)\n",
    "            inputs_embeds = torch.cat([prefix_emb, code_emb[:, :-1, :]], dim=1)\n",
    "\n",
    "            labels = input_ids.clone()\n",
    "            labels[:, 0] = -100\n",
    "\n",
    "            outputs = gpt2(inputs_embeds=inputs_embeds, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            if use_sdf_loss:\n",
    "                try:\n",
    "                    sdf_gt = batch.get(\"sdf\", None)\n",
    "                    if sdf_gt is not None:\n",
    "                        pred_code = gpt2.generate(inputs_embeds=prefix_emb, max_length=256)[0]\n",
    "                        sdf_loss = mesh_loss(pred_code, sdf_gt)\n",
    "                        loss += 0.1 * sdf_loss\n",
    "                        # wandb.log({\"train/sdf_loss\": sdf_loss.item()})\n",
    "                except Exception as e:\n",
    "                    print(\"SDF loss failed\", e)\n",
    "\n",
    "            # wandb.log({\"train/loss\": loss.item(), \"epoch\": epoch, \"step\": step})\n",
    "\n",
    "            if use_feedback:\n",
    "                pred_code = tokenizer.decode(\n",
    "                    gpt2.generate(inputs_embeds=prefix_emb, max_length=256)[0],\n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                reward = 0.0\n",
    "                if not is_valid_python(pred_code):\n",
    "                    reward -= 0.5\n",
    "                if not is_executable(pred_code):\n",
    "                    reward -= 0.5\n",
    "                try:\n",
    "                    gt_code = batch[\"code\"][0]\n",
    "                    iou = get_iou_best(gt_code, pred_code)\n",
    "                    wandb.log({\"train/feedback_iou\": iou})\n",
    "                    reward += iou\n",
    "                except:\n",
    "                    reward -= 0.5\n",
    "                if use_ppo:\n",
    "                    ppo_loss = ppo_trainer.step(images, reward)\n",
    "                    loss += 0.1 * ppo_loss\n",
    "                    wandb.log({\"train/ppo_loss\": ppo_loss.item()})\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1} Step {step} Loss: {loss.item():.4f}\")\n",
    "\n",
    "# ------------------- Entry Points -------------------\n",
    "def train_baseline(train_loader):\n",
    "    global use_feedback, use_sdf_loss, use_ppo\n",
    "    use_feedback = False\n",
    "    use_sdf_loss = False\n",
    "    use_ppo = False\n",
    "    train(train_loader)\n",
    "    torch.save(gpt2.state_dict(), \"baseline_gpt2.pth\")\n",
    "    torch.save(proj.state_dict(), \"baseline_proj.pth\")\n",
    "\n",
    "def train_enhanced(train_loader):\n",
    "    global use_feedback, use_sdf_loss, use_ppo\n",
    "    use_feedback = True\n",
    "    use_sdf_loss = True\n",
    "    use_ppo = True\n",
    "    train(train_loader)\n",
    "    torch.save(gpt2.state_dict(), \"enhanced_gpt2.pth\")\n",
    "    torch.save(proj.state_dict(), \"enhanced_proj.pth\")\n",
    "\n",
    "# ------------------- Evaluation -------------------\n",
    "def generate_code_from_image(image, max_length=256):\n",
    "    gpt2.eval()\n",
    "    proj.eval()\n",
    "    with torch.no_grad():\n",
    "        processed = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        clip_out = clip_model.get_image_features(**processed)\n",
    "        img_emb = proj(clip_out)\n",
    "        prefix_emb = img_emb.unsqueeze(1)\n",
    "\n",
    "        generated = gpt2.generate(\n",
    "            inputs_embeds=prefix_emb,\n",
    "            attention_mask=torch.ones(prefix_emb.shape[:-1], dtype=torch.long, device=device),\n",
    "            max_length=max_length,\n",
    "            num_beams=5,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "        return tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "def evaluate_syntax_rate_dataset(dataset):\n",
    "    generated_codes = {}\n",
    "    for i, sample in enumerate(dataset):\n",
    "        image = sample[\"image\"].unsqueeze(0).to(device)\n",
    "        code = generate_code_from_image(image)\n",
    "        generated_codes[f\"sample_{i}\"] = code\n",
    "    return evaluate_syntax_rate(generated_codes)\n",
    "\n",
    "def evaluate_iou_dataset(dataset):\n",
    "    scores = []\n",
    "    for i, sample in enumerate(dataset):\n",
    "        image = sample[\"image\"].unsqueeze(0).to(device)\n",
    "        gt_code = sample[\"code\"]\n",
    "        pred_code = generate_code_from_image(image)\n",
    "        try:\n",
    "            iou = get_iou_best(gt_code, pred_code)\n",
    "        except Exception:\n",
    "            iou = 0.0\n",
    "        scores.append(iou)\n",
    "    return sum(scores) / len(scores) if scores else 0.0\n",
    "\n",
    "def evaluate_baseline_vs_enhanced(dataset):\n",
    "    print(\"=== [Baseline Model] ===\")\n",
    "    gpt2.load_state_dict(torch.load(\"baseline_gpt2.pth\", map_location=device))\n",
    "    proj.load_state_dict(torch.load(\"baseline_proj.pth\", map_location=device))\n",
    "    vsr = evaluate_syntax_rate_dataset(dataset)\n",
    "    print(f\"Valid Syntax Rate: {vsr['vsr']:.3f}\")\n",
    "    iou_score = evaluate_iou_dataset(dataset)\n",
    "    print(f\"Mean IOU: {iou_score:.3f}\")\n",
    "\n",
    "    print(\"\\n=== [Enhanced Model] ===\")\n",
    "    gpt2.load_state_dict(torch.load(\"enhanced_gpt2.pth\", map_location=device))\n",
    "    proj.load_state_dict(torch.load(\"enhanced_proj.pth\", map_location=device))\n",
    "    vsr = evaluate_syntax_rate_dataset(dataset)\n",
    "    print(f\"Valid Syntax Rate: {vsr['vsr']:.3f}\")\n",
    "    iou_score = evaluate_iou_dataset(dataset)\n",
    "    print(f\"Mean IOU: {iou_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 0 Loss: 9.7545\n",
      "Epoch 1 Step 10 Loss: 8.1049\n",
      "Epoch 1 Step 20 Loss: 5.8820\n",
      "Epoch 1 Step 30 Loss: 4.8052\n",
      "Epoch 1 Step 40 Loss: 4.8572\n",
      "Epoch 1 Step 50 Loss: 3.4424\n",
      "Epoch 1 Step 60 Loss: 3.1419\n",
      "Epoch 1 Step 70 Loss: 2.7112\n",
      "Epoch 1 Step 80 Loss: 1.7764\n",
      "Epoch 1 Step 90 Loss: 1.2486\n",
      "Epoch 1 Step 100 Loss: 1.1238\n",
      "Epoch 1 Step 110 Loss: 1.2595\n",
      "Epoch 1 Step 120 Loss: 0.8214\n",
      "Epoch 1 Step 130 Loss: 0.5899\n",
      "Epoch 1 Step 140 Loss: 0.6288\n",
      "Epoch 1 Step 150 Loss: 0.6511\n",
      "Epoch 1 Step 160 Loss: 0.9431\n",
      "Epoch 1 Step 170 Loss: 0.6655\n",
      "Epoch 1 Step 180 Loss: 0.5017\n",
      "Epoch 1 Step 190 Loss: 0.5071\n",
      "Epoch 1 Step 200 Loss: 0.5804\n",
      "Epoch 1 Step 210 Loss: 0.7913\n",
      "Epoch 1 Step 220 Loss: 0.5411\n",
      "Epoch 1 Step 230 Loss: 0.7341\n",
      "Epoch 1 Step 240 Loss: 0.2968\n",
      "Epoch 1 Step 250 Loss: 0.5749\n",
      "Epoch 1 Step 260 Loss: 0.5423\n",
      "Epoch 1 Step 270 Loss: 0.3573\n",
      "Epoch 1 Step 280 Loss: 0.4015\n",
      "Epoch 1 Step 290 Loss: 0.4285\n",
      "Epoch 1 Step 300 Loss: 0.3992\n",
      "Epoch 1 Step 310 Loss: 0.5604\n",
      "Epoch 1 Step 320 Loss: 0.5135\n",
      "Epoch 1 Step 330 Loss: 0.2996\n",
      "Epoch 1 Step 340 Loss: 0.4464\n",
      "Epoch 1 Step 350 Loss: 0.3672\n",
      "Epoch 1 Step 360 Loss: 0.7087\n",
      "Epoch 1 Step 370 Loss: 0.3619\n",
      "Epoch 1 Step 380 Loss: 0.2831\n",
      "Epoch 1 Step 390 Loss: 0.4618\n",
      "Epoch 1 Step 400 Loss: 0.5764\n",
      "Epoch 1 Step 410 Loss: 0.3661\n",
      "Epoch 1 Step 420 Loss: 0.5677\n",
      "Epoch 1 Step 430 Loss: 0.4354\n",
      "Epoch 1 Step 440 Loss: 0.3656\n",
      "Epoch 1 Step 450 Loss: 0.2334\n",
      "Epoch 1 Step 460 Loss: 0.2461\n",
      "Epoch 1 Step 470 Loss: 0.2586\n",
      "Epoch 1 Step 480 Loss: 0.4865\n",
      "Epoch 1 Step 490 Loss: 0.5175\n",
      "Epoch 1 Step 500 Loss: 0.3684\n",
      "Epoch 1 Step 510 Loss: 0.2110\n",
      "Epoch 1 Step 520 Loss: 0.4573\n",
      "Epoch 1 Step 530 Loss: 0.5432\n",
      "Epoch 1 Step 540 Loss: 0.5357\n",
      "Epoch 1 Step 550 Loss: 0.3695\n",
      "Epoch 1 Step 560 Loss: 0.6244\n",
      "Epoch 1 Step 570 Loss: 0.6183\n",
      "Epoch 1 Step 580 Loss: 0.5269\n",
      "Epoch 1 Step 590 Loss: 0.3153\n",
      "Epoch 1 Step 600 Loss: 0.6195\n",
      "Epoch 1 Step 610 Loss: 0.6849\n",
      "Epoch 1 Step 620 Loss: 0.3966\n",
      "Epoch 1 Step 630 Loss: 0.4281\n",
      "Epoch 1 Step 640 Loss: 0.8048\n",
      "Epoch 1 Step 650 Loss: 0.5490\n",
      "Epoch 1 Step 660 Loss: 0.6507\n",
      "Epoch 1 Step 670 Loss: 0.4155\n",
      "Epoch 1 Step 680 Loss: 0.3863\n",
      "Epoch 1 Step 690 Loss: 0.5153\n",
      "Epoch 1 Step 700 Loss: 0.2976\n",
      "Epoch 1 Step 710 Loss: 0.5575\n",
      "Epoch 1 Step 720 Loss: 0.4235\n",
      "Epoch 1 Step 730 Loss: 0.5176\n",
      "Epoch 1 Step 740 Loss: 0.4332\n",
      "Epoch 1 Step 750 Loss: 0.4349\n",
      "Epoch 1 Step 760 Loss: 0.2911\n",
      "Epoch 1 Step 770 Loss: 0.4802\n",
      "Epoch 1 Step 780 Loss: 0.4597\n",
      "Epoch 1 Step 790 Loss: 0.5743\n",
      "Epoch 1 Step 800 Loss: 0.5565\n",
      "Epoch 1 Step 810 Loss: 0.2793\n",
      "Epoch 1 Step 820 Loss: 0.4437\n",
      "Epoch 1 Step 830 Loss: 0.6659\n",
      "Epoch 1 Step 840 Loss: 0.9519\n",
      "Epoch 1 Step 850 Loss: 0.2588\n",
      "Epoch 1 Step 860 Loss: 0.2221\n",
      "Epoch 1 Step 870 Loss: 0.4751\n",
      "Epoch 1 Step 880 Loss: 0.2622\n",
      "Epoch 1 Step 890 Loss: 0.5509\n",
      "Epoch 1 Step 900 Loss: 0.3555\n",
      "Epoch 1 Step 910 Loss: 0.5303\n",
      "Epoch 1 Step 920 Loss: 0.5242\n",
      "Epoch 1 Step 930 Loss: 0.4582\n",
      "Epoch 1 Step 940 Loss: 0.2594\n",
      "Epoch 1 Step 950 Loss: 0.5220\n",
      "Epoch 1 Step 960 Loss: 0.3051\n",
      "Epoch 1 Step 970 Loss: 0.6393\n",
      "Epoch 1 Step 980 Loss: 0.6929\n",
      "Epoch 1 Step 990 Loss: 0.6988\n",
      "Epoch 1 Step 1000 Loss: 0.5626\n",
      "Epoch 1 Step 1010 Loss: 0.6714\n",
      "Epoch 1 Step 1020 Loss: 0.4004\n",
      "Epoch 1 Step 1030 Loss: 0.4230\n",
      "Epoch 1 Step 1040 Loss: 0.6028\n",
      "Epoch 1 Step 1050 Loss: 0.5414\n",
      "Epoch 1 Step 1060 Loss: 0.2385\n",
      "Epoch 1 Step 1070 Loss: 0.4305\n",
      "Epoch 1 Step 1080 Loss: 0.3493\n",
      "Epoch 1 Step 1090 Loss: 0.7780\n",
      "Epoch 1 Step 1100 Loss: 0.2929\n",
      "Epoch 1 Step 1110 Loss: 0.7282\n",
      "Epoch 1 Step 1120 Loss: 0.4408\n",
      "Epoch 1 Step 1130 Loss: 0.5097\n",
      "Epoch 1 Step 1140 Loss: 0.4155\n",
      "Epoch 1 Step 1150 Loss: 0.3664\n",
      "Epoch 1 Step 1160 Loss: 0.3352\n",
      "Epoch 1 Step 1170 Loss: 0.1943\n",
      "Epoch 1 Step 1180 Loss: 0.2705\n",
      "Epoch 1 Step 1190 Loss: 0.4894\n",
      "Epoch 1 Step 1200 Loss: 0.3235\n",
      "Epoch 1 Step 1210 Loss: 0.9392\n",
      "Epoch 1 Step 1220 Loss: 0.5499\n",
      "Epoch 1 Step 1230 Loss: 0.6368\n",
      "Epoch 1 Step 1240 Loss: 0.6954\n",
      "Epoch 1 Step 1250 Loss: 0.3024\n",
      "Epoch 1 Step 1260 Loss: 0.2818\n",
      "Epoch 1 Step 1270 Loss: 0.5400\n",
      "Epoch 1 Step 1280 Loss: 0.9306\n",
      "Epoch 1 Step 1290 Loss: 0.2653\n",
      "Epoch 1 Step 1300 Loss: 0.6520\n",
      "Epoch 1 Step 1310 Loss: 0.3382\n",
      "Epoch 1 Step 1320 Loss: 0.3936\n",
      "Epoch 1 Step 1330 Loss: 0.3553\n",
      "Epoch 1 Step 1340 Loss: 0.4861\n",
      "Epoch 1 Step 1350 Loss: 1.0647\n",
      "Epoch 1 Step 1360 Loss: 0.1324\n",
      "Epoch 1 Step 1370 Loss: 0.2853\n",
      "Epoch 1 Step 1380 Loss: 0.1487\n",
      "Epoch 1 Step 1390 Loss: 0.4332\n",
      "Epoch 1 Step 1400 Loss: 0.2828\n",
      "Epoch 1 Step 1410 Loss: 0.7466\n",
      "Epoch 1 Step 1420 Loss: 0.3829\n",
      "Epoch 1 Step 1430 Loss: 0.3815\n"
     ]
    }
   ],
   "source": [
    "train_baseline(train_loader)\n",
    "train_enhanced(train_loader)\n",
    "evaluate_baseline_vs_enhanced(test_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
