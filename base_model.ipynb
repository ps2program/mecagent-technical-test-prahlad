{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### CADQuery Code Generation - Notebook Summary\n",
    "\n",
    "#### 1. Objective\n",
    "\n",
    "Generate CadQuery code from rendered 2D images using a vision-to-language model.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Dataset\n",
    "\n",
    "* `CADCODER/GenCAD-Code`\n",
    "* 147K pairs of rendered image and corresponding CadQuery script.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Baseline Model\n",
    "\n",
    "* **CLIP (ViT-B/16)** as image encoder (frozen)\n",
    "* **GPT2** as language decoder (frozen)\n",
    "* **Projection Head**: MLP to map CLIP features to GPT2 token embeddings\n",
    "* **Training**: CrossEntropy loss on code tokens\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Enhanced Model\n",
    "\n",
    "* **Beam search decoding**: `num_beams=5`, `temperature=0.7`, `top_p=0.95`\n",
    "* **Feedback signal**: Valid syntax, executability, and geometry IOU\n",
    "* **PPO Reinforcement**: Reward-weighted training using PPO\n",
    "* **Optional geometry loss**: SDF/mesh loss placeholder\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Evaluation Metrics\n",
    "\n",
    "* **Valid Syntax Rate (VSR)**\n",
    "* **IOU\\_best**: 3D shape overlap via voxelization\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. Outcome\n",
    "\n",
    "* Both baseline and enhanced models implemented\n",
    "* Reinforcement and feedback-ready pipeline\n",
    "* Evaluation setup for fair comparison\n",
    "\n",
    "---\n",
    "\n",
    "> Full implementation details, design choices, and results are documented in the [README](./README.md).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses/mesh_loss.py\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from cadquery import exporters\n",
    "from cadquery import cq\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "def dummy_sdf_generator(code):\n",
    "    # Placeholder: create a fake voxel grid for now\n",
    "    # In practice, this should voxelize the actual CAD geometry\n",
    "    return torch.rand((32, 32, 32))\n",
    "\n",
    "def mesh_loss(pred_code: str, gt_sdf: torch.Tensor) -> torch.Tensor:\n",
    "    try:\n",
    "        sdf_pred = dummy_sdf_generator(pred_code)\n",
    "        sdf_pred = sdf_pred.to(gt_sdf.device)\n",
    "        return F.mse_loss(sdf_pred, gt_sdf)\n",
    "    except Exception as e:\n",
    "        print(\"Mesh loss generation failed:\", e)\n",
    "        return torch.tensor(0.0, requires_grad=True, device=gt_sdf.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rl/ppo.py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# You may need to import clip_model and clip_processor externally or pass as argument\n",
    "\n",
    "class PPOTrainer:\n",
    "    def __init__(self, model, proj, tokenizer, reward_fn, clip_model, clip_processor, device):\n",
    "        self.model = model\n",
    "        self.proj = proj\n",
    "        self.tokenizer = tokenizer\n",
    "        self.reward_fn = reward_fn\n",
    "        self.clip_model = clip_model\n",
    "        self.clip_processor = clip_processor\n",
    "        self.device = device\n",
    "\n",
    "        self.optimizer = AdamW(list(model.parameters()) + list(proj.parameters()), lr=1e-5)\n",
    "\n",
    "    def step(self, images, reward):\n",
    "        self.model.eval()\n",
    "        self.proj.eval()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            processed = self.clip_processor(images=images, return_tensors=\"pt\", padding=True).to(self.device)\n",
    "            clip_out = self.clip_model.get_image_features(**processed)\n",
    "            img_emb = self.proj(clip_out)\n",
    "            prefix_emb = img_emb.unsqueeze(1)\n",
    "\n",
    "        input_embeds = prefix_emb\n",
    "        outputs = self.model(inputs_embeds=input_embeds, labels=None)\n",
    "        logits = outputs.logits\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        # Simple scalar reward signal applied to log-probabilities\n",
    "        loss = -reward * log_probs.mean()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (\n",
    "    CLIPProcessor, CLIPModel,\n",
    "    GPT2Tokenizer, GPT2LMHeadModel,\n",
    "    AdamW, get_linear_schedule_with_warmup\n",
    ")\n",
    "from cadquery import exporters\n",
    "from metrics.best_iou import get_iou_best\n",
    "from metrics.valid_syntax_rate import evaluate_syntax_rate\n",
    "from datasets import load_dataset\n",
    "import ast\n",
    "import wandb\n",
    "from PIL import Image\n",
    "import io\n",
    "from torchvision import transforms\n",
    "\n",
    "# Optional: For SDF/mesh-based loss and PPO\n",
    "# from losses.mesh_loss import mesh_loss\n",
    "# from rl.ppo import PPOTrainer\n",
    "\n",
    "# ------------------- WandB -------------------\n",
    "# wandb.init(project=\"cadquery-codegen\", name=\"clip-gpt2-enhanced\")\n",
    "\n",
    "# ------------------- Models -------------------\n",
    "class ClipToGPT2Improved(nn.Module):\n",
    "    def __init__(self, clip_dim, gpt2_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(clip_dim, 4 * gpt2_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(4 * gpt2_dim, gpt2_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.proj(x)\n",
    "\n",
    "# ------------------- Utilities -------------------\n",
    "def is_valid_python(code):\n",
    "    try:\n",
    "        ast.parse(code)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def is_executable(code):\n",
    "    try:\n",
    "        exec_globals = {}\n",
    "        exec(code, exec_globals)\n",
    "        return exec_globals.get(\"result\", None) is not None\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# ------------------- Dataset -------------------\n",
    "class ImageCodeDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=256):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        img_data = item[\"image\"]\n",
    "\n",
    "        if isinstance(img_data, dict) and \"bytes\" in img_data:\n",
    "            image = Image.open(io.BytesIO(img_data[\"bytes\"])).convert(\"RGB\")\n",
    "        elif isinstance(img_data, str):\n",
    "            image = Image.open(img_data).convert(\"RGB\")\n",
    "        elif isinstance(img_data, Image.Image):\n",
    "            image = img_data.convert(\"RGB\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown image format: {type(img_data)}\")\n",
    "\n",
    "        image_tensor = self.transform(image)\n",
    "\n",
    "        code = item[\"cadquery\"]\n",
    "        code_ids = self.tokenizer(code, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            \"image\": image_tensor,\n",
    "            \"input_ids\": code_ids[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": code_ids[\"attention_mask\"].squeeze(0),\n",
    "            \"code\": code\n",
    "        }\n",
    "\n",
    "# ------------------- Load Models -------------------\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "gpt2.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "proj = ClipToGPT2Improved(clip_model.config.projection_dim, gpt2.config.n_embd)\n",
    "\n",
    "# ------------------- Device Setup -------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpt2.to(device)\n",
    "clip_model.to(device)\n",
    "proj.to(device)\n",
    "\n",
    "# ------------------- Optimizer -------------------\n",
    "optimizer = AdamW(list(gpt2.parameters()) + list(proj.parameters()), lr=5e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=1000)\n",
    "\n",
    "# ------------------- Flags -------------------\n",
    "use_feedback = True\n",
    "use_sdf_loss = True\n",
    "use_ppo = True\n",
    "ppo_trainer = PPOTrainer(\n",
    "    model=gpt2,\n",
    "    proj=proj,\n",
    "    tokenizer=tokenizer,\n",
    "    reward_fn=get_iou_best,\n",
    "    clip_model=clip_model,\n",
    "    clip_processor=clip_processor,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------- Load Dataset -------------------\n",
    "ds = load_dataset(\"CADCODER/GenCAD-Code\", cache_dir=\"/tmp/hf_cache\")\n",
    "train_dataset = ImageCodeDataset(ds[\"train\"], tokenizer)\n",
    "test_dataset = ImageCodeDataset(ds[\"test\"], tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "# ------------------- Training -------------------\n",
    "def train(train_loader):\n",
    "    gpt2.train()\n",
    "    proj.train()\n",
    "\n",
    "    for epoch in range(5):\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            images = batch[\"image\"].to(device)\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                processed = clip_processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n",
    "                clip_out = clip_model.get_image_features(**processed)\n",
    "\n",
    "            img_emb = proj(clip_out)\n",
    "            prefix_emb = img_emb.unsqueeze(1)\n",
    "\n",
    "            code_emb = gpt2.transformer.wte(input_ids)\n",
    "            inputs_embeds = torch.cat([prefix_emb, code_emb[:, :-1, :]], dim=1)\n",
    "\n",
    "            labels = input_ids.clone()\n",
    "            labels[:, 0] = -100\n",
    "\n",
    "            outputs = gpt2(inputs_embeds=inputs_embeds, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            if use_sdf_loss:\n",
    "                try:\n",
    "                    sdf_gt = batch.get(\"sdf\", None)\n",
    "                    if sdf_gt is not None:\n",
    "                        pred_code = gpt2.generate(inputs_embeds=prefix_emb, max_length=256)[0]\n",
    "                        sdf_loss = mesh_loss(pred_code, sdf_gt)\n",
    "                        loss += 0.1 * sdf_loss\n",
    "                        # wandb.log({\"train/sdf_loss\": sdf_loss.item()})\n",
    "                except Exception as e:\n",
    "                    print(\"SDF loss failed\", e)\n",
    "\n",
    "            # wandb.log({\"train/loss\": loss.item(), \"epoch\": epoch, \"step\": step})\n",
    "\n",
    "            if use_feedback:\n",
    "                pred_code = tokenizer.decode(\n",
    "                    gpt2.generate(inputs_embeds=prefix_emb, max_length=256)[0],\n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                reward = 0.0\n",
    "                if not is_valid_python(pred_code):\n",
    "                    reward -= 0.5\n",
    "                if not is_executable(pred_code):\n",
    "                    reward -= 0.5\n",
    "                try:\n",
    "                    gt_code = batch[\"code\"][0]\n",
    "                    iou = get_iou_best(gt_code, pred_code)\n",
    "                    wandb.log({\"train/feedback_iou\": iou})\n",
    "                    reward += iou\n",
    "                except:\n",
    "                    reward -= 0.5\n",
    "                if use_ppo:\n",
    "                    ppo_loss = ppo_trainer.step(images, reward)\n",
    "                    loss += 0.1 * ppo_loss\n",
    "                    wandb.log({\"train/ppo_loss\": ppo_loss.item()})\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1} Step {step} Loss: {loss.item():.4f}\")\n",
    "\n",
    "# ------------------- Entry Points -------------------\n",
    "def train_baseline(train_loader):\n",
    "    global use_feedback, use_sdf_loss, use_ppo\n",
    "    use_feedback = False\n",
    "    use_sdf_loss = False\n",
    "    use_ppo = False\n",
    "    train(train_loader)\n",
    "    torch.save(gpt2.state_dict(), \"baseline_gpt2.pth\")\n",
    "    torch.save(proj.state_dict(), \"baseline_proj.pth\")\n",
    "\n",
    "def train_enhanced(train_loader):\n",
    "    global use_feedback, use_sdf_loss, use_ppo\n",
    "    use_feedback = True\n",
    "    use_sdf_loss = True\n",
    "    use_ppo = True\n",
    "    train(train_loader)\n",
    "    torch.save(gpt2.state_dict(), \"enhanced_gpt2.pth\")\n",
    "    torch.save(proj.state_dict(), \"enhanced_proj.pth\")\n",
    "\n",
    "# ------------------- Evaluation -------------------\n",
    "def generate_code_from_image(image, max_length=256):\n",
    "    gpt2.eval()\n",
    "    proj.eval()\n",
    "    with torch.no_grad():\n",
    "        processed = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        clip_out = clip_model.get_image_features(**processed)\n",
    "        img_emb = proj(clip_out)\n",
    "        prefix_emb = img_emb.unsqueeze(1)\n",
    "\n",
    "        generated = gpt2.generate(\n",
    "            inputs_embeds=prefix_emb,\n",
    "            attention_mask=torch.ones(prefix_emb.shape[:-1], dtype=torch.long, device=device),\n",
    "            max_length=max_length,\n",
    "            num_beams=5,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "        return tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "def evaluate_syntax_rate_dataset(dataset):\n",
    "    generated_codes = {}\n",
    "    for i, sample in enumerate(dataset):\n",
    "        image = sample[\"image\"].unsqueeze(0).to(device)\n",
    "        code = generate_code_from_image(image)\n",
    "        generated_codes[f\"sample_{i}\"] = code\n",
    "    return evaluate_syntax_rate(generated_codes)\n",
    "\n",
    "def evaluate_iou_dataset(dataset):\n",
    "    scores = []\n",
    "    for i, sample in enumerate(dataset):\n",
    "        image = sample[\"image\"].unsqueeze(0).to(device)\n",
    "        gt_code = sample[\"code\"]\n",
    "        pred_code = generate_code_from_image(image)\n",
    "        try:\n",
    "            iou = get_iou_best(gt_code, pred_code)\n",
    "        except Exception:\n",
    "            iou = 0.0\n",
    "        scores.append(iou)\n",
    "    return sum(scores) / len(scores) if scores else 0.0\n",
    "\n",
    "def evaluate_baseline_vs_enhanced(dataset):\n",
    "    print(\"=== [Baseline Model] ===\")\n",
    "    gpt2.load_state_dict(torch.load(\"baseline_gpt2.pth\", map_location=device))\n",
    "    proj.load_state_dict(torch.load(\"baseline_proj.pth\", map_location=device))\n",
    "    vsr = evaluate_syntax_rate_dataset(dataset)\n",
    "    print(f\"Valid Syntax Rate: {vsr['vsr']:.3f}\")\n",
    "    iou_score = evaluate_iou_dataset(dataset)\n",
    "    print(f\"Mean IOU: {iou_score:.3f}\")\n",
    "\n",
    "    print(\"\\n=== [Enhanced Model] ===\")\n",
    "    gpt2.load_state_dict(torch.load(\"enhanced_gpt2.pth\", map_location=device))\n",
    "    proj.load_state_dict(torch.load(\"enhanced_proj.pth\", map_location=device))\n",
    "    vsr = evaluate_syntax_rate_dataset(dataset)\n",
    "    print(f\"Valid Syntax Rate: {vsr['vsr']:.3f}\")\n",
    "    iou_score = evaluate_iou_dataset(dataset)\n",
    "    print(f\"Mean IOU: {iou_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 0 Loss: 9.7545\n",
      "Epoch 1 Step 10 Loss: 8.1049\n",
      "Epoch 1 Step 20 Loss: 5.8820\n",
      "Epoch 1 Step 30 Loss: 4.8052\n",
      "Epoch 1 Step 40 Loss: 4.8572\n",
      "Epoch 1 Step 50 Loss: 3.4424\n",
      "Epoch 1 Step 60 Loss: 3.1419\n",
      "Epoch 1 Step 70 Loss: 2.7112\n",
      "Epoch 1 Step 80 Loss: 1.7764\n",
      "Epoch 1 Step 90 Loss: 1.2486\n",
      "Epoch 1 Step 100 Loss: 1.1238\n",
      "Epoch 1 Step 110 Loss: 1.2595\n",
      "Epoch 1 Step 120 Loss: 0.8214\n",
      "Epoch 1 Step 130 Loss: 0.5899\n",
      "Epoch 1 Step 140 Loss: 0.6288\n",
      "Epoch 1 Step 150 Loss: 0.6511\n",
      "Epoch 1 Step 160 Loss: 0.9431\n",
      "Epoch 1 Step 170 Loss: 0.6655\n",
      "Epoch 1 Step 180 Loss: 0.5017\n",
      "Epoch 1 Step 190 Loss: 0.5071\n",
      "Epoch 1 Step 200 Loss: 0.5804\n",
      "Epoch 1 Step 210 Loss: 0.7913\n",
      "Epoch 1 Step 220 Loss: 0.5411\n",
      "Epoch 1 Step 230 Loss: 0.7341\n",
      "Epoch 1 Step 240 Loss: 0.2968\n",
      "Epoch 1 Step 250 Loss: 0.5749\n",
      "Epoch 1 Step 260 Loss: 0.5423\n",
      "Epoch 1 Step 270 Loss: 0.3573\n",
      "Epoch 1 Step 280 Loss: 0.4015\n",
      "Epoch 1 Step 290 Loss: 0.4285\n",
      "Epoch 1 Step 300 Loss: 0.3992\n",
      "Epoch 1 Step 310 Loss: 0.5604\n",
      "Epoch 1 Step 320 Loss: 0.5135\n",
      "Epoch 1 Step 330 Loss: 0.2996\n",
      "Epoch 1 Step 340 Loss: 0.4464\n",
      "Epoch 1 Step 350 Loss: 0.3672\n",
      "Epoch 1 Step 360 Loss: 0.7087\n",
      "Epoch 1 Step 370 Loss: 0.3619\n",
      "Epoch 1 Step 380 Loss: 0.2831\n",
      "Epoch 1 Step 390 Loss: 0.4618\n",
      "Epoch 1 Step 400 Loss: 0.5764\n",
      "Epoch 1 Step 410 Loss: 0.3661\n",
      "Epoch 1 Step 420 Loss: 0.5677\n",
      "Epoch 1 Step 430 Loss: 0.4354\n",
      "Epoch 1 Step 440 Loss: 0.3656\n",
      "Epoch 1 Step 450 Loss: 0.2334\n",
      "Epoch 1 Step 460 Loss: 0.2461\n",
      "Epoch 1 Step 470 Loss: 0.2586\n",
      "Epoch 1 Step 480 Loss: 0.4865\n",
      "Epoch 1 Step 490 Loss: 0.5175\n",
      "Epoch 1 Step 500 Loss: 0.3684\n",
      "Epoch 1 Step 510 Loss: 0.2110\n",
      "Epoch 1 Step 520 Loss: 0.4573\n",
      "Epoch 1 Step 530 Loss: 0.5432\n",
      "Epoch 1 Step 540 Loss: 0.5357\n",
      "Epoch 1 Step 550 Loss: 0.3695\n",
      "Epoch 1 Step 560 Loss: 0.6244\n",
      "Epoch 1 Step 570 Loss: 0.6183\n",
      "Epoch 1 Step 580 Loss: 0.5269\n",
      "Epoch 1 Step 590 Loss: 0.3153\n",
      "Epoch 1 Step 600 Loss: 0.6195\n",
      "Epoch 1 Step 610 Loss: 0.6849\n",
      "Epoch 1 Step 620 Loss: 0.3966\n",
      "Epoch 1 Step 630 Loss: 0.4281\n",
      "Epoch 1 Step 640 Loss: 0.8048\n",
      "Epoch 1 Step 650 Loss: 0.5490\n",
      "Epoch 1 Step 660 Loss: 0.6507\n",
      "Epoch 1 Step 670 Loss: 0.4155\n",
      "Epoch 1 Step 680 Loss: 0.3863\n",
      "Epoch 1 Step 690 Loss: 0.5153\n",
      "Epoch 1 Step 700 Loss: 0.2976\n",
      "Epoch 1 Step 710 Loss: 0.5575\n",
      "Epoch 1 Step 720 Loss: 0.4235\n",
      "Epoch 1 Step 730 Loss: 0.5176\n",
      "Epoch 1 Step 740 Loss: 0.4332\n",
      "Epoch 1 Step 750 Loss: 0.4349\n",
      "Epoch 1 Step 760 Loss: 0.2911\n",
      "Epoch 1 Step 770 Loss: 0.4802\n",
      "Epoch 1 Step 780 Loss: 0.4597\n",
      "Epoch 1 Step 790 Loss: 0.5743\n",
      "Epoch 1 Step 800 Loss: 0.5565\n",
      "Epoch 1 Step 810 Loss: 0.2793\n",
      "Epoch 1 Step 820 Loss: 0.4437\n",
      "Epoch 1 Step 830 Loss: 0.6659\n",
      "Epoch 1 Step 840 Loss: 0.9519\n",
      "Epoch 1 Step 850 Loss: 0.2588\n",
      "Epoch 1 Step 860 Loss: 0.2221\n",
      "Epoch 1 Step 870 Loss: 0.4751\n",
      "Epoch 1 Step 880 Loss: 0.2622\n",
      "Epoch 1 Step 890 Loss: 0.5509\n",
      "Epoch 1 Step 900 Loss: 0.3555\n",
      "Epoch 1 Step 910 Loss: 0.5303\n",
      "Epoch 1 Step 920 Loss: 0.5242\n",
      "Epoch 1 Step 930 Loss: 0.4582\n",
      "Epoch 1 Step 940 Loss: 0.2594\n",
      "Epoch 1 Step 950 Loss: 0.5220\n",
      "Epoch 1 Step 960 Loss: 0.3051\n",
      "Epoch 1 Step 970 Loss: 0.6393\n",
      "Epoch 1 Step 980 Loss: 0.6929\n",
      "Epoch 1 Step 990 Loss: 0.6988\n",
      "Epoch 1 Step 1000 Loss: 0.5626\n",
      "Epoch 1 Step 1010 Loss: 0.6714\n",
      "Epoch 1 Step 1020 Loss: 0.4004\n",
      "Epoch 1 Step 1030 Loss: 0.4230\n",
      "Epoch 1 Step 1040 Loss: 0.6028\n",
      "Epoch 1 Step 1050 Loss: 0.5414\n",
      "Epoch 1 Step 1060 Loss: 0.2385\n",
      "Epoch 1 Step 1070 Loss: 0.4305\n",
      "Epoch 1 Step 1080 Loss: 0.3493\n",
      "Epoch 1 Step 1090 Loss: 0.7780\n",
      "Epoch 1 Step 1100 Loss: 0.2929\n",
      "Epoch 1 Step 1110 Loss: 0.7282\n",
      "Epoch 1 Step 1120 Loss: 0.4408\n",
      "Epoch 1 Step 1130 Loss: 0.5097\n",
      "Epoch 1 Step 1140 Loss: 0.4155\n",
      "Epoch 1 Step 1150 Loss: 0.3664\n",
      "Epoch 1 Step 1160 Loss: 0.3352\n",
      "Epoch 1 Step 1170 Loss: 0.1943\n",
      "Epoch 1 Step 1180 Loss: 0.2705\n",
      "Epoch 1 Step 1190 Loss: 0.4894\n",
      "Epoch 1 Step 1200 Loss: 0.3235\n",
      "Epoch 1 Step 1210 Loss: 0.9392\n",
      "Epoch 1 Step 1220 Loss: 0.5499\n",
      "Epoch 1 Step 1230 Loss: 0.6368\n",
      "Epoch 1 Step 1240 Loss: 0.6954\n",
      "Epoch 1 Step 1250 Loss: 0.3024\n",
      "Epoch 1 Step 1260 Loss: 0.2818\n",
      "Epoch 1 Step 1270 Loss: 0.5400\n",
      "Epoch 1 Step 1280 Loss: 0.9306\n",
      "Epoch 1 Step 1290 Loss: 0.2653\n",
      "Epoch 1 Step 1300 Loss: 0.6520\n",
      "Epoch 1 Step 1310 Loss: 0.3382\n",
      "Epoch 1 Step 1320 Loss: 0.3936\n",
      "Epoch 1 Step 1330 Loss: 0.3553\n",
      "Epoch 1 Step 1340 Loss: 0.4861\n",
      "Epoch 1 Step 1350 Loss: 1.0647\n",
      "Epoch 1 Step 1360 Loss: 0.1324\n",
      "Epoch 1 Step 1370 Loss: 0.2853\n",
      "Epoch 1 Step 1380 Loss: 0.1487\n",
      "Epoch 1 Step 1390 Loss: 0.4332\n",
      "Epoch 1 Step 1400 Loss: 0.2828\n",
      "Epoch 1 Step 1410 Loss: 0.7466\n",
      "Epoch 1 Step 1420 Loss: 0.3829\n",
      "Epoch 1 Step 1430 Loss: 0.3815\n",
      "Epoch 1 Step 1440 Loss: 0.3650\n",
      "Epoch 1 Step 1450 Loss: 0.4631\n",
      "Epoch 1 Step 1460 Loss: 0.2807\n",
      "Epoch 1 Step 1470 Loss: 0.8611\n",
      "Epoch 1 Step 1480 Loss: 0.4616\n",
      "Epoch 1 Step 1490 Loss: 0.5676\n",
      "Epoch 1 Step 1500 Loss: 0.4461\n",
      "Epoch 1 Step 1510 Loss: 0.5277\n",
      "Epoch 1 Step 1520 Loss: 0.2323\n",
      "Epoch 1 Step 1530 Loss: 0.4098\n",
      "Epoch 1 Step 1540 Loss: 0.4561\n",
      "Epoch 1 Step 1550 Loss: 0.3926\n",
      "Epoch 1 Step 1560 Loss: 0.3885\n",
      "Epoch 1 Step 1570 Loss: 0.5278\n",
      "Epoch 1 Step 1580 Loss: 0.6342\n",
      "Epoch 1 Step 1590 Loss: 0.3418\n",
      "Epoch 1 Step 1600 Loss: 0.2330\n",
      "Epoch 1 Step 1610 Loss: 0.2386\n",
      "Epoch 1 Step 1620 Loss: 0.7944\n",
      "Epoch 1 Step 1630 Loss: 0.3492\n",
      "Epoch 1 Step 1640 Loss: 0.4605\n",
      "Epoch 1 Step 1650 Loss: 0.6665\n",
      "Epoch 1 Step 1660 Loss: 0.6691\n",
      "Epoch 1 Step 1670 Loss: 0.5034\n",
      "Epoch 1 Step 1680 Loss: 0.3636\n",
      "Epoch 1 Step 1690 Loss: 0.2997\n",
      "Epoch 1 Step 1700 Loss: 0.3302\n",
      "Epoch 1 Step 1710 Loss: 0.7689\n",
      "Epoch 1 Step 1720 Loss: 0.4375\n",
      "Epoch 1 Step 1730 Loss: 0.5471\n",
      "Epoch 1 Step 1740 Loss: 0.3927\n",
      "Epoch 1 Step 1750 Loss: 0.3371\n",
      "Epoch 1 Step 1760 Loss: 0.4342\n",
      "Epoch 1 Step 1770 Loss: 0.8641\n",
      "Epoch 1 Step 1780 Loss: 0.4179\n",
      "Epoch 1 Step 1790 Loss: 0.6694\n",
      "Epoch 1 Step 1800 Loss: 0.3166\n",
      "Epoch 1 Step 1810 Loss: 0.2807\n",
      "Epoch 1 Step 1820 Loss: 0.6533\n",
      "Epoch 1 Step 1830 Loss: 0.6965\n",
      "Epoch 1 Step 1840 Loss: 0.4178\n",
      "Epoch 1 Step 1850 Loss: 0.5361\n",
      "Epoch 1 Step 1860 Loss: 0.4155\n",
      "Epoch 1 Step 1870 Loss: 0.4610\n",
      "Epoch 1 Step 1880 Loss: 0.5585\n",
      "Epoch 1 Step 1890 Loss: 0.3136\n",
      "Epoch 1 Step 1900 Loss: 0.9270\n",
      "Epoch 1 Step 1910 Loss: 0.7016\n",
      "Epoch 1 Step 1920 Loss: 0.4726\n",
      "Epoch 1 Step 1930 Loss: 0.4555\n",
      "Epoch 1 Step 1940 Loss: 0.4124\n",
      "Epoch 1 Step 1950 Loss: 0.4933\n",
      "Epoch 1 Step 1960 Loss: 0.4017\n",
      "Epoch 1 Step 1970 Loss: 0.6627\n",
      "Epoch 1 Step 1980 Loss: 0.2558\n",
      "Epoch 1 Step 1990 Loss: 0.7660\n",
      "Epoch 1 Step 2000 Loss: 0.2450\n",
      "Epoch 1 Step 2010 Loss: 0.6395\n",
      "Epoch 1 Step 2020 Loss: 0.3286\n",
      "Epoch 1 Step 2030 Loss: 0.3914\n",
      "Epoch 1 Step 2040 Loss: 0.4558\n",
      "Epoch 1 Step 2050 Loss: 0.5581\n",
      "Epoch 1 Step 2060 Loss: 0.2782\n",
      "Epoch 1 Step 2070 Loss: 0.7041\n",
      "Epoch 1 Step 2080 Loss: 0.4337\n",
      "Epoch 1 Step 2090 Loss: 0.2928\n",
      "Epoch 1 Step 2100 Loss: 0.6379\n",
      "Epoch 1 Step 2110 Loss: 0.4925\n",
      "Epoch 1 Step 2120 Loss: 0.7927\n",
      "Epoch 1 Step 2130 Loss: 0.4474\n",
      "Epoch 1 Step 2140 Loss: 0.4696\n",
      "Epoch 1 Step 2150 Loss: 0.2406\n",
      "Epoch 1 Step 2160 Loss: 0.4186\n",
      "Epoch 1 Step 2170 Loss: 0.3459\n",
      "Epoch 1 Step 2180 Loss: 0.5519\n",
      "Epoch 1 Step 2190 Loss: 0.4206\n",
      "Epoch 1 Step 2200 Loss: 0.3031\n",
      "Epoch 1 Step 2210 Loss: 0.4121\n",
      "Epoch 1 Step 2220 Loss: 0.3483\n",
      "Epoch 1 Step 2230 Loss: 0.5336\n",
      "Epoch 1 Step 2240 Loss: 0.3669\n",
      "Epoch 1 Step 2250 Loss: 0.5032\n",
      "Epoch 1 Step 2260 Loss: 0.4630\n",
      "Epoch 1 Step 2270 Loss: 0.6819\n",
      "Epoch 1 Step 2280 Loss: 0.3512\n",
      "Epoch 1 Step 2290 Loss: 0.6017\n",
      "Epoch 1 Step 2300 Loss: 0.6097\n",
      "Epoch 1 Step 2310 Loss: 0.3135\n",
      "Epoch 1 Step 2320 Loss: 0.5096\n",
      "Epoch 1 Step 2330 Loss: 0.8131\n",
      "Epoch 1 Step 2340 Loss: 0.5401\n",
      "Epoch 1 Step 2350 Loss: 0.5692\n",
      "Epoch 1 Step 2360 Loss: 0.1901\n",
      "Epoch 1 Step 2370 Loss: 0.4478\n",
      "Epoch 1 Step 2380 Loss: 0.3893\n",
      "Epoch 1 Step 2390 Loss: 0.3283\n",
      "Epoch 1 Step 2400 Loss: 0.5874\n",
      "Epoch 1 Step 2410 Loss: 0.5339\n",
      "Epoch 1 Step 2420 Loss: 0.4147\n",
      "Epoch 1 Step 2430 Loss: 0.6081\n",
      "Epoch 1 Step 2440 Loss: 0.2274\n",
      "Epoch 1 Step 2450 Loss: 0.3093\n",
      "Epoch 1 Step 2460 Loss: 0.4873\n",
      "Epoch 1 Step 2470 Loss: 0.2507\n",
      "Epoch 1 Step 2480 Loss: 0.4423\n",
      "Epoch 1 Step 2490 Loss: 0.5910\n",
      "Epoch 1 Step 2500 Loss: 0.2028\n",
      "Epoch 1 Step 2510 Loss: 0.7333\n",
      "Epoch 1 Step 2520 Loss: 0.7171\n",
      "Epoch 1 Step 2530 Loss: 0.3641\n",
      "Epoch 1 Step 2540 Loss: 0.4606\n",
      "Epoch 1 Step 2550 Loss: 0.4515\n",
      "Epoch 1 Step 2560 Loss: 0.3521\n",
      "Epoch 1 Step 2570 Loss: 0.8029\n",
      "Epoch 1 Step 2580 Loss: 0.4513\n",
      "Epoch 1 Step 2590 Loss: 0.7279\n",
      "Epoch 1 Step 2600 Loss: 0.5320\n",
      "Epoch 1 Step 2610 Loss: 0.5589\n",
      "Epoch 1 Step 2620 Loss: 0.3683\n",
      "Epoch 1 Step 2630 Loss: 0.5967\n",
      "Epoch 1 Step 2640 Loss: 0.7834\n",
      "Epoch 1 Step 2650 Loss: 0.6946\n",
      "Epoch 1 Step 2660 Loss: 0.5039\n",
      "Epoch 1 Step 2670 Loss: 0.6799\n",
      "Epoch 1 Step 2680 Loss: 0.4416\n",
      "Epoch 1 Step 2690 Loss: 0.4628\n",
      "Epoch 1 Step 2700 Loss: 0.5870\n",
      "Epoch 1 Step 2710 Loss: 0.5354\n",
      "Epoch 1 Step 2720 Loss: 0.1574\n",
      "Epoch 1 Step 2730 Loss: 0.5196\n",
      "Epoch 1 Step 2740 Loss: 0.2969\n",
      "Epoch 1 Step 2750 Loss: 0.5474\n",
      "Epoch 1 Step 2760 Loss: 0.5077\n",
      "Epoch 1 Step 2770 Loss: 0.6458\n",
      "Epoch 1 Step 2780 Loss: 0.2606\n",
      "Epoch 1 Step 2790 Loss: 0.3753\n",
      "Epoch 1 Step 2800 Loss: 0.6545\n",
      "Epoch 1 Step 2810 Loss: 0.5178\n",
      "Epoch 1 Step 2820 Loss: 0.3652\n",
      "Epoch 1 Step 2830 Loss: 0.3804\n",
      "Epoch 1 Step 2840 Loss: 0.3296\n",
      "Epoch 1 Step 2850 Loss: 0.7609\n",
      "Epoch 1 Step 2860 Loss: 0.3654\n",
      "Epoch 1 Step 2870 Loss: 0.7197\n",
      "Epoch 1 Step 2880 Loss: 0.5514\n",
      "Epoch 1 Step 2890 Loss: 0.5345\n",
      "Epoch 1 Step 2900 Loss: 0.6693\n",
      "Epoch 1 Step 2910 Loss: 0.7624\n",
      "Epoch 1 Step 2920 Loss: 0.6804\n",
      "Epoch 1 Step 2930 Loss: 0.3604\n",
      "Epoch 1 Step 2940 Loss: 0.7316\n",
      "Epoch 1 Step 2950 Loss: 0.5594\n",
      "Epoch 1 Step 2960 Loss: 0.3307\n",
      "Epoch 1 Step 2970 Loss: 0.3427\n",
      "Epoch 1 Step 2980 Loss: 0.5275\n",
      "Epoch 1 Step 2990 Loss: 0.5434\n",
      "Epoch 1 Step 3000 Loss: 0.5746\n",
      "Epoch 1 Step 3010 Loss: 0.5992\n",
      "Epoch 1 Step 3020 Loss: 0.5412\n",
      "Epoch 1 Step 3030 Loss: 0.6600\n",
      "Epoch 1 Step 3040 Loss: 1.0613\n",
      "Epoch 1 Step 3050 Loss: 0.4921\n",
      "Epoch 1 Step 3060 Loss: 0.4326\n",
      "Epoch 1 Step 3070 Loss: 0.2688\n",
      "Epoch 1 Step 3080 Loss: 0.4742\n",
      "Epoch 1 Step 3090 Loss: 0.5476\n",
      "Epoch 1 Step 3100 Loss: 0.3262\n",
      "Epoch 1 Step 3110 Loss: 0.5067\n",
      "Epoch 1 Step 3120 Loss: 0.9703\n",
      "Epoch 1 Step 3130 Loss: 0.4801\n",
      "Epoch 1 Step 3140 Loss: 0.4017\n",
      "Epoch 1 Step 3150 Loss: 0.3925\n",
      "Epoch 1 Step 3160 Loss: 0.5347\n",
      "Epoch 1 Step 3170 Loss: 0.4976\n",
      "Epoch 1 Step 3180 Loss: 0.2735\n",
      "Epoch 1 Step 3190 Loss: 0.2736\n",
      "Epoch 1 Step 3200 Loss: 0.5552\n",
      "Epoch 1 Step 3210 Loss: 0.3183\n",
      "Epoch 1 Step 3220 Loss: 0.5837\n",
      "Epoch 1 Step 3230 Loss: 0.7674\n",
      "Epoch 1 Step 3240 Loss: 0.9356\n",
      "Epoch 1 Step 3250 Loss: 0.6492\n",
      "Epoch 1 Step 3260 Loss: 0.4547\n",
      "Epoch 1 Step 3270 Loss: 0.2958\n",
      "Epoch 1 Step 3280 Loss: 0.3192\n",
      "Epoch 1 Step 3290 Loss: 0.7858\n",
      "Epoch 1 Step 3300 Loss: 1.0010\n",
      "Epoch 1 Step 3310 Loss: 0.4430\n",
      "Epoch 1 Step 3320 Loss: 0.2807\n",
      "Epoch 1 Step 3330 Loss: 0.2721\n",
      "Epoch 1 Step 3340 Loss: 0.5131\n",
      "Epoch 1 Step 3350 Loss: 0.2659\n",
      "Epoch 1 Step 3360 Loss: 0.4964\n",
      "Epoch 1 Step 3370 Loss: 0.5633\n",
      "Epoch 1 Step 3380 Loss: 0.5462\n",
      "Epoch 1 Step 3390 Loss: 0.4847\n",
      "Epoch 1 Step 3400 Loss: 0.5782\n",
      "Epoch 1 Step 3410 Loss: 0.7390\n",
      "Epoch 1 Step 3420 Loss: 0.1911\n",
      "Epoch 1 Step 3430 Loss: 0.6517\n",
      "Epoch 1 Step 3440 Loss: 0.2256\n",
      "Epoch 1 Step 3450 Loss: 0.2777\n",
      "Epoch 1 Step 3460 Loss: 0.7493\n",
      "Epoch 1 Step 3470 Loss: 0.2024\n",
      "Epoch 1 Step 3480 Loss: 0.4047\n",
      "Epoch 1 Step 3490 Loss: 0.4109\n",
      "Epoch 1 Step 3500 Loss: 0.5290\n",
      "Epoch 1 Step 3510 Loss: 0.5369\n",
      "Epoch 1 Step 3520 Loss: 0.2851\n",
      "Epoch 1 Step 3530 Loss: 0.4100\n",
      "Epoch 1 Step 3540 Loss: 0.6669\n",
      "Epoch 1 Step 3550 Loss: 0.5453\n",
      "Epoch 1 Step 3560 Loss: 0.8423\n",
      "Epoch 1 Step 3570 Loss: 0.4983\n",
      "Epoch 1 Step 3580 Loss: 0.4118\n",
      "Epoch 1 Step 3590 Loss: 0.2848\n",
      "Epoch 1 Step 3600 Loss: 0.4147\n",
      "Epoch 1 Step 3610 Loss: 0.7022\n",
      "Epoch 1 Step 3620 Loss: 0.2370\n",
      "Epoch 1 Step 3630 Loss: 0.8701\n",
      "Epoch 1 Step 3640 Loss: 0.5035\n",
      "Epoch 1 Step 3650 Loss: 0.5488\n",
      "Epoch 1 Step 3660 Loss: 0.7865\n",
      "Epoch 1 Step 3670 Loss: 0.2959\n",
      "Epoch 1 Step 3680 Loss: 0.5930\n",
      "Epoch 1 Step 3690 Loss: 0.7991\n",
      "Epoch 1 Step 3700 Loss: 0.3256\n",
      "Epoch 1 Step 3710 Loss: 0.2816\n",
      "Epoch 1 Step 3720 Loss: 0.4470\n",
      "Epoch 1 Step 3730 Loss: 0.2649\n",
      "Epoch 1 Step 3740 Loss: 0.8995\n",
      "Epoch 1 Step 3750 Loss: 0.1998\n",
      "Epoch 1 Step 3760 Loss: 0.2687\n",
      "Epoch 1 Step 3770 Loss: 0.3807\n",
      "Epoch 1 Step 3780 Loss: 0.5743\n",
      "Epoch 1 Step 3790 Loss: 0.3180\n",
      "Epoch 1 Step 3800 Loss: 0.7922\n",
      "Epoch 1 Step 3810 Loss: 0.3563\n",
      "Epoch 1 Step 3820 Loss: 0.8612\n",
      "Epoch 1 Step 3830 Loss: 0.6552\n",
      "Epoch 1 Step 3840 Loss: 0.4127\n",
      "Epoch 1 Step 3850 Loss: 0.3252\n",
      "Epoch 1 Step 3860 Loss: 0.2343\n",
      "Epoch 1 Step 3870 Loss: 0.6071\n",
      "Epoch 1 Step 3880 Loss: 0.6283\n",
      "Epoch 1 Step 3890 Loss: 0.2938\n",
      "Epoch 1 Step 3900 Loss: 0.5161\n",
      "Epoch 1 Step 3910 Loss: 0.8613\n",
      "Epoch 1 Step 3920 Loss: 0.5384\n",
      "Epoch 1 Step 3930 Loss: 0.2591\n",
      "Epoch 1 Step 3940 Loss: 0.5525\n",
      "Epoch 1 Step 3950 Loss: 0.5228\n",
      "Epoch 1 Step 3960 Loss: 0.3392\n",
      "Epoch 1 Step 3970 Loss: 0.6574\n",
      "Epoch 1 Step 3980 Loss: 0.8953\n",
      "Epoch 1 Step 3990 Loss: 0.6294\n",
      "Epoch 1 Step 4000 Loss: 0.7578\n",
      "Epoch 1 Step 4010 Loss: 0.4875\n",
      "Epoch 1 Step 4020 Loss: 0.5753\n",
      "Epoch 1 Step 4030 Loss: 0.5363\n",
      "Epoch 1 Step 4040 Loss: 0.6249\n",
      "Epoch 1 Step 4050 Loss: 0.8063\n",
      "Epoch 1 Step 4060 Loss: 0.4958\n",
      "Epoch 1 Step 4070 Loss: 0.4864\n",
      "Epoch 1 Step 4080 Loss: 0.4400\n",
      "Epoch 1 Step 4090 Loss: 0.6584\n",
      "Epoch 1 Step 4100 Loss: 0.4453\n",
      "Epoch 1 Step 4110 Loss: 0.4092\n",
      "Epoch 1 Step 4120 Loss: 0.4255\n",
      "Epoch 1 Step 4130 Loss: 0.3018\n",
      "Epoch 1 Step 4140 Loss: 0.5241\n",
      "Epoch 1 Step 4150 Loss: 0.4935\n",
      "Epoch 1 Step 4160 Loss: 0.8171\n",
      "Epoch 1 Step 4170 Loss: 0.3773\n",
      "Epoch 1 Step 4180 Loss: 0.4488\n",
      "Epoch 1 Step 4190 Loss: 0.3416\n",
      "Epoch 1 Step 4200 Loss: 0.3172\n",
      "Epoch 1 Step 4210 Loss: 0.5231\n",
      "Epoch 1 Step 4220 Loss: 0.4342\n",
      "Epoch 1 Step 4230 Loss: 0.2841\n",
      "Epoch 1 Step 4240 Loss: 0.5573\n",
      "Epoch 1 Step 4250 Loss: 0.7175\n",
      "Epoch 1 Step 4260 Loss: 0.8902\n",
      "Epoch 1 Step 4270 Loss: 0.6438\n",
      "Epoch 1 Step 4280 Loss: 0.2568\n",
      "Epoch 1 Step 4290 Loss: 0.6241\n",
      "Epoch 1 Step 4300 Loss: 0.3587\n",
      "Epoch 1 Step 4310 Loss: 0.4415\n",
      "Epoch 1 Step 4320 Loss: 0.5745\n",
      "Epoch 1 Step 4330 Loss: 0.3909\n",
      "Epoch 1 Step 4340 Loss: 0.2691\n",
      "Epoch 1 Step 4350 Loss: 0.2844\n",
      "Epoch 1 Step 4360 Loss: 1.1383\n",
      "Epoch 1 Step 4370 Loss: 0.3412\n",
      "Epoch 1 Step 4380 Loss: 0.6091\n",
      "Epoch 1 Step 4390 Loss: 0.2555\n",
      "Epoch 1 Step 4400 Loss: 0.6248\n",
      "Epoch 1 Step 4410 Loss: 0.2643\n",
      "Epoch 1 Step 4420 Loss: 0.2440\n",
      "Epoch 1 Step 4430 Loss: 0.2575\n",
      "Epoch 1 Step 4440 Loss: 0.6057\n",
      "Epoch 1 Step 4450 Loss: 0.3620\n",
      "Epoch 1 Step 4460 Loss: 0.8279\n",
      "Epoch 1 Step 4470 Loss: 0.3351\n",
      "Epoch 1 Step 4480 Loss: 0.7009\n",
      "Epoch 1 Step 4490 Loss: 0.2861\n",
      "Epoch 1 Step 4500 Loss: 0.2578\n",
      "Epoch 1 Step 4510 Loss: 0.4730\n",
      "Epoch 1 Step 4520 Loss: 0.3755\n",
      "Epoch 1 Step 4530 Loss: 0.5095\n",
      "Epoch 1 Step 4540 Loss: 0.5982\n",
      "Epoch 1 Step 4550 Loss: 0.4864\n",
      "Epoch 1 Step 4560 Loss: 0.4868\n",
      "Epoch 1 Step 4570 Loss: 0.2259\n",
      "Epoch 1 Step 4580 Loss: 0.2114\n",
      "Epoch 1 Step 4590 Loss: 0.6458\n",
      "Epoch 1 Step 4600 Loss: 0.3462\n",
      "Epoch 1 Step 4610 Loss: 0.7353\n",
      "Epoch 1 Step 4620 Loss: 0.3873\n",
      "Epoch 1 Step 4630 Loss: 0.3503\n",
      "Epoch 1 Step 4640 Loss: 0.2831\n",
      "Epoch 1 Step 4650 Loss: 0.5055\n",
      "Epoch 1 Step 4660 Loss: 0.3360\n",
      "Epoch 1 Step 4670 Loss: 0.3994\n",
      "Epoch 1 Step 4680 Loss: 0.3033\n",
      "Epoch 1 Step 4690 Loss: 0.4999\n",
      "Epoch 1 Step 4700 Loss: 0.7627\n",
      "Epoch 1 Step 4710 Loss: 0.8057\n",
      "Epoch 1 Step 4720 Loss: 0.3406\n",
      "Epoch 1 Step 4730 Loss: 0.3303\n",
      "Epoch 1 Step 4740 Loss: 0.4204\n",
      "Epoch 1 Step 4750 Loss: 0.7784\n",
      "Epoch 1 Step 4760 Loss: 0.5064\n",
      "Epoch 1 Step 4770 Loss: 0.6482\n",
      "Epoch 1 Step 4780 Loss: 0.5930\n",
      "Epoch 1 Step 4790 Loss: 0.5741\n",
      "Epoch 1 Step 4800 Loss: 0.4417\n",
      "Epoch 1 Step 4810 Loss: 0.3531\n",
      "Epoch 1 Step 4820 Loss: 0.3398\n",
      "Epoch 1 Step 4830 Loss: 0.5925\n",
      "Epoch 1 Step 4840 Loss: 0.5605\n",
      "Epoch 1 Step 4850 Loss: 0.2933\n",
      "Epoch 1 Step 4860 Loss: 0.4019\n",
      "Epoch 1 Step 4870 Loss: 0.5058\n",
      "Epoch 1 Step 4880 Loss: 0.2098\n",
      "Epoch 1 Step 4890 Loss: 0.6159\n",
      "Epoch 1 Step 4900 Loss: 0.4164\n",
      "Epoch 1 Step 4910 Loss: 0.4472\n",
      "Epoch 1 Step 4920 Loss: 0.3723\n",
      "Epoch 1 Step 4930 Loss: 0.3914\n",
      "Epoch 1 Step 4940 Loss: 0.3688\n",
      "Epoch 1 Step 4950 Loss: 0.7112\n",
      "Epoch 1 Step 4960 Loss: 0.4588\n",
      "Epoch 1 Step 4970 Loss: 1.2400\n",
      "Epoch 1 Step 4980 Loss: 0.4471\n",
      "Epoch 1 Step 4990 Loss: 0.3435\n"
     ]
    }
   ],
   "source": [
    "train_baseline(train_loader)\n",
    "train_enhanced(train_loader)\n",
    "evaluate_baseline_vs_enhanced(test_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
